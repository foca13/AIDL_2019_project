# -*- coding: utf-8 -*-
"""Final_project_siamese_cosine_sim_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YpNq2CdGP72DgsKFtR7aq0Yg1O5SxKx-
"""

from google.colab import drive

drive.mount('/content/gdrive')
root_path = "/content/gdrive/My Drive/Final project AIDL/"

# imports

import numpy as np
import torch
import torchvision
import os.path as osp
from pathlib import Path
from torch.utils.data import dataset
from PIL import Image
import PIL
import os
import random
from torch import nn
from torchvision.models import vgg16_bn
from torchvision import transforms
from tqdm import tqdm_notebook

# Setting the seed
seed = 1

torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

np.random.seed(seed)
random.seed(seed)

"""**CREATE DATASET CLASS**"""

class Args:
  frontal_images_directories = root_path+"/dataset-cfp/Protocol/image_list_F.txt"
  profile_images_directories = root_path+"/dataset-cfp/Protocol/image_list_P.txt"
  split_main_directory = root_path+"/dataset-cfp/Protocol/Split"
  split_traindata = ["01", "02", "03", "04", "05", "06"]
  split_valdata = ["07", "08"]
  split_testdata = ["09", "10"]
  dataset_root = root_path
  dataset= "CFPDataset"

class CFPDataset(dataset.Dataset):
    def __init__(self, path, args, img_transforms=None, dataset_root="",
                 split="train", input_size=(224, 224)):
        super().__init__()

        self.data = []
        self.split = split

        self.load(path, args)

        print("Dataset loaded")
        print("{0} samples in the {1} dataset".format(len(self.data),
                                                      self.split))
        self.transforms = img_transforms
        self.dataset_root = dataset_root
        self.input_size = input_size

    def load(self, path, args):

        # read directories for frontal images
        lines = open(args.frontal_images_directories).readlines()
        idx = 0
        directories_frontal_images = []
        while idx < len(lines):
            x = lines[idx].strip().split()
            directories_frontal_images.append(x)
            idx += 1

        # read directories for profile images
        lines = open(args.profile_images_directories).readlines()
        idx = 0
        directories_profile_images = []
        while idx < len(lines):
            x = lines[idx].strip().split()
            directories_profile_images.append(x)
            idx += 1

        # read same and different pairs of images and save at dictionary
        self.data = []
        for i in path:
            ff_diff_file = osp.join(args.split_main_directory, 'FF', i,
                                    'diff.txt')
            lines = open(ff_diff_file).readlines()
            idx = 0
            while idx < int(len(lines)/1):
                img_pair = lines[idx].strip().split(',')
                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]
                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]
                pair_tag = -1
                d = {
                    "img1_path": img1_dir,
                    "img2_path": img2_dir,
                    "pair_tag": pair_tag
                }
                self.data.append(d)
                idx += 1

            ff_same_file = osp.join(args.split_main_directory, 'FF', i,
                                    'same.txt')
            lines = open(ff_same_file).readlines()
            idx = 0
            while idx < int(len(lines)/1):
                img_pair = lines[idx].strip().split(',')
                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]
                img2_dir = directories_frontal_images[int(img_pair[1])-1][1]
                pair_tag = 1
                d = {
                    "img1_path": img1_dir,
                    "img2_path": img2_dir,
                    "pair_tag": pair_tag
                }
                self.data.append(d)
                idx += 1

            fp_diff_file = osp.join(args.split_main_directory, 'FP', i,
                                    'diff.txt')
            lines = open(fp_diff_file).readlines()
            idx = 0
            while idx < int(len(lines)/1):
                img_pair = lines[idx].strip().split(',')
                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]
                img2_dir = directories_profile_images[int(img_pair[1])-1][1]
                pair_tag = -1
                d = {
                    "img1_path": img1_dir,
                    "img2_path": img2_dir,
                    "pair_tag": pair_tag
                }
                self.data.append(d)
                idx += 1

            fp_same_file = osp.join(args.split_main_directory, 'FP', i,
                                    'same.txt')
            lines = open(fp_same_file).readlines()
            idx = 0
            while idx < int(len(lines)/1):
                img_pair = lines[idx].strip().split(',')
                img1_dir = directories_frontal_images[int(img_pair[0])-1][1]
                img2_dir = directories_profile_images[int(img_pair[1])-1][1]
                pair_tag = 1
                d = {
                    "img1_path": img1_dir,
                    "img2_path": img2_dir,
                    "pair_tag": pair_tag
                }
                self.data.append(d)
                idx += 1

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        d = self.data[index]
        image1_path = osp.join(self.dataset_root, 'dataset-cfp', d[
            'img1_path'])
        image2_path = osp.join(self.dataset_root, 'dataset-cfp', d[
            'img2_path'])
        image1 = Image.open(image1_path).convert('RGB')
        image2 = Image.open(image2_path).convert('RGB')
        tag = d['pair_tag']
        if self.transforms is not None:
            # this converts from (HxWxC) to (CxHxW) as well
            img1 = self.transforms(image1)
            img2 = self.transforms(image2)

        return img1, img2, tag

"""**CREATE SIAMESE WITH COSINE DISTANCE**"""

def l2norm(x):
  x = x / torch.sqrt(torch.sum(x**2, dim=-1, keepdim=True))
  return x

class SiameseCosine(nn.Module):
    """
    Siamese network
    """
    def __init__(self, pretrained=False):
        super(SiameseCosine, self).__init__()

        self.feat_conv = vgg16_bn(pretrained=pretrained).features
        self.feat_linear = vgg16_bn(pretrained=pretrained).classifier[0]


    def forward(self, img1, img2):
        feat_1 = self.feat_conv(img1).view(img1.size(0),-1)
        
        feat_1 = self.feat_linear(feat_1)
        
        feat_1 = l2norm(feat_1)
        
        feat_2 = self.feat_conv(img2).view(img2.size(0),-1)
        feat_2 = self.feat_linear(feat_2)

        feat_2 = l2norm(feat_2)
        
        return feat_1, feat_2

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, count=1):
        self.val = val
        self.sum += val*count
        self.count += count
        self.avg = self.sum / self.count

"""**TRAINING LOOP**"""

def train(model, loader, loss_criterion, optimizer):
    model.train()
    loader = tqdm_notebook(loader, desc='MiniBatch: ')
    losses = AverageMeter()
    optimizer.zero_grad()

    for i, (img1, img2, gt) in enumerate(loader, start = 0):
        
        # This calls getitem
        
        img1 =  img1.to('cuda:0')
        img2 = img2.to('cuda:0')
        gt = gt.float().to('cuda:0')  #label
        
     
        out1, out2 = model(img1, img2) #inputs images to model, executes model, returns features
   
        loss = loss_criterion(out1, out2, gt) #calculates loss
        
        loss.backward() #upgrades gradients
        
        batch_size = img1.size(0)
        losses.update(loss.item(), batch_size)

        optimizer.step()
        optimizer.zero_grad()

        torch.cuda.empty_cache()
        
    return losses.avg

"""**VALIDATION LOOP**"""

def val(model, loader, loss_criterion):
    model.eval()
    loader = tqdm_notebook(loader, desc='MiniBatch: ')
    
    losses = AverageMeter()

    with torch.no_grad():
      for i, (img1, img2, gt) in enumerate(loader, start = 0):

            img1 =  img1.to('cuda:0')
            img2 = img2.to('cuda:0')
            gt = gt.float().to('cuda:0')
            
            batch_size = img1.size(0)

            out1, out2 = model(img1, img2)

            loss = loss_criterion(out1, out2, gt)

            losses.update(loss.item(), batch_size)
 
            torch.cuda.empty_cache()
            
    return losses.avg

"""**HYPERPARAMETERS**"""

pretrained = True
n_epochs = 20

lr = 1e-3
betas = (0.9, 0.999)

wd = 0.0005

bz = 16

data_aug = True 

momentum = float(0.9)

check_point_name = 'gdrive/My Drive/Final project AIDL/results/best_epoch.pth.tar'

"""**MAIN**"""

#datasetTrain = torchvision.datasets.CIFAR10('./cifar10/train', train=True, download=True, transform=transform)
args = Args()
train_transform=None
if data_aug == False:
  train_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]) #ToTensor transform the image from RGB to Tensor

else:
  train_transform = transforms.Compose([transforms.Resize((224, 224)), 
                                        transforms.RandomHorizontalFlip(), 
                                        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR), 
                                        transforms.ToTensor()])



val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

datasetTrain = CFPDataset(args.split_traindata, args, split="train", img_transforms=train_transform, dataset_root=osp.expanduser(args.dataset_root))

datasetVal= CFPDataset(args.split_valdata, args, split="val", img_transforms=val_transform, dataset_root=osp.expanduser(args.dataset_root))                          


trainloader = torch.utils.data.DataLoader(datasetTrain, batch_size=bz, shuffle=True, drop_last=True, num_workers=4)
valloader = torch.utils.data.DataLoader(datasetVal, batch_size=bz,shuffle=False, num_workers=4)


# create the network

model = SiameseCosine(pretrained=pretrained)
  
model = model.to('cuda:0')

# define the loss
loss_criterion = nn.CosineEmbeddingLoss(margin=0.5).cuda()

# define the optimizer
#optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)


all_train_loss = []

all_val_loss = []

best_loss = 1
best_epoch = 0

for epoch in tqdm_notebook(range(n_epochs), desc='Epoch: '):  
  
    train_loss = train(model, trainloader, loss_criterion, optimizer)
    
    all_train_loss.append(train_loss)

                    
    print("Train Epoch: ",epoch, " Loss: ",train_loss)

    val_loss = val(model, valloader, loss_criterion)
    
    print(val_loss)
    
    all_val_loss.append(val_loss)

    if val_loss < best_loss:
        best_loss = val_loss
        best_epoch = epoch
      
        checkpoint = {
          'epoch' : epoch,
          'model' : model.state_dict(),
          'loss' : best_loss,
        }
        torch.save(checkpoint, check_point_name) 
    
    save_epochs = {
        'loss_epoch_train': all_train_loss,
        'loss_epoch_val': all_val_loss
    }
    
    torch.save(save_epochs,'gdrive/My Drive/Final project AIDL/results/loss_acc_per_epoch_11')
        
    print("Val Epoch: ",epoch, " Loss: ", val_loss)
    
    del val_loss, train_loss
    

print("Best Epoch: ",best_epoch, "Best loss: ", best_loss)

"""**PLOT RESULTS**"""

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['axes.grid'] = False

plt.plot(all_train_loss,marker='o', label="Train")
plt.plot(all_val_loss,marker='o', label="Val")
plt.xlabel("N epoch")
plt.ylabel("Loss")
plt.title("Loss")
plt.grid(True)
plt.legend();

"""**TEST**"""

# load checkpoint with best results in the model

checkpoint = torch.load(check_point_name)

model.load_state_dict(check_point_name['model'])

datasetVal= CFPDataset(args.split_testdata, args, split="test", img_transforms=val_transform, dataset_root=osp.expanduser(args.dataset_root))                          
                         
testloader = torch.utils.data.DataLoader(datasetTest, batch_size=bz ,shuffle=False, num_workers=4)